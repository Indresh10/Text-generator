{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import sys\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import book file it's in utf-8 format \n",
    "file = open(\"wonderland.txt\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the words\n",
    "def tokenize_words(input):\n",
    "    input = input.lower() ##making every char to lowercase \n",
    "    tokenizer=RegexpTokenizer(r'\\w+ ') ##using regexptokenizer to select word char only(A-Z,1-9,_) \n",
    "    tokens=tokenizer.tokenize(input) ##tokenizing the input\n",
    "    filtered=filter(lambda token: token not in stopwords.words(\"english\"),tokens) \n",
    "    ##filtering out the words who doesn't have meaning like is,the etc.\n",
    "    return \"\".join(filtered) ## returning that data\n",
    "\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##creating th sorted list of each characters\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "##giving each chars a numeric value\n",
    "char_to_num = dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of chars: 97761\n",
      "Total vocab: 29\n"
     ]
    }
   ],
   "source": [
    "input_len=len(processed_inputs) ##total no. of characters\n",
    "vocal_len=len(chars) ##total no. of unique characters\n",
    "print(\"Total no of chars:\",input_len)\n",
    "print(\"Total vocab:\",vocal_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 97661\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "x_data = []\n",
    "y_data = []\n",
    "# creating sequences\n",
    "for i in range(0, input_len - seq_len,1):\n",
    "    seq_in=processed_inputs[i:(i+seq_len)] ## 1 to 100 \n",
    "    seq_out=processed_inputs[(i+seq_len)] ## 101\n",
    "    ## converting the char to num using dict and adding it x_data and y_data\n",
    "    x_data.append([char_to_num[x] for x in seq_in])  \n",
    "    y_data.append(char_to_num[seq_out])\n",
    "## getting the total no. of patterns\n",
    "patterns=len(x_data) \n",
    "print(\"Total Patterns:\",patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the x_data to (Samples, time stamps ,features)\n",
    "X=np.reshape(x_data, (patterns, seq_len, 1))\n",
    "X=X/float(vocal_len) ##normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np_utils.to_categorical(y_data) ##one hot encoding of y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##crating LSTM Model\n",
    "model=Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]), return_sequences=True))##input and LSTM layer\n",
    "model.add(Dropout(0.2))##dropout layer\n",
    "model.add(LSTM(128))## again an LSTM layer\n",
    "model.add(Dropout(0.2))## dropout layer\n",
    "model.add(Dense(y.shape[1], activation=\"softmax\"))## final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\") ##compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Checkpoint for model\n",
    "backup_file = \"model_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(backup_file, monitor=\"loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.8162\n",
      "Epoch 00001: loss improved from inf to 2.81618, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 860s 2s/step - loss: 2.8162\n",
      "Epoch 2/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.6492\n",
      "Epoch 00002: loss improved from 2.81618 to 2.64925, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 935s 2s/step - loss: 2.6492\n",
      "Epoch 3/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.4025\n",
      "Epoch 00003: loss improved from 2.64925 to 2.40247, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 930s 2s/step - loss: 2.4025\n",
      "Epoch 4/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.2642\n",
      "Epoch 00004: loss improved from 2.40247 to 2.26417, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 995s 3s/step - loss: 2.2642\n",
      "Epoch 5/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.1676\n",
      "Epoch 00005: loss improved from 2.26417 to 2.16762, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 984s 3s/step - loss: 2.1676\n",
      "Epoch 6/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.0932\n",
      "Epoch 00006: loss improved from 2.16762 to 2.09322, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1074s 3s/step - loss: 2.0932\n",
      "Epoch 7/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 2.0311\n",
      "Epoch 00007: loss improved from 2.09322 to 2.03108, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1178s 3s/step - loss: 2.0311\n",
      "Epoch 8/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.9791\n",
      "Epoch 00008: loss improved from 2.03108 to 1.97909, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1184s 3s/step - loss: 1.9791\n",
      "Epoch 9/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.9367\n",
      "Epoch 00009: loss improved from 1.97909 to 1.93672, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1176s 3s/step - loss: 1.9367\n",
      "Epoch 10/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.9032\n",
      "Epoch 00010: loss improved from 1.93672 to 1.90320, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1149s 3s/step - loss: 1.9032\n",
      "Epoch 11/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.8693\n",
      "Epoch 00011: loss improved from 1.90320 to 1.86927, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1183s 3s/step - loss: 1.8693\n",
      "Epoch 12/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.8439\n",
      "Epoch 00012: loss improved from 1.86927 to 1.84394, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1134s 3s/step - loss: 1.8439\n",
      "Epoch 13/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.8167\n",
      "Epoch 00013: loss improved from 1.84394 to 1.81673, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 937s 2s/step - loss: 1.8167\n",
      "Epoch 14/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.7958\n",
      "Epoch 00014: loss improved from 1.81673 to 1.79582, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 1002s 3s/step - loss: 1.7958\n",
      "Epoch 15/15\n",
      "382/382 [==============================] - ETA: 0s - loss: 1.7740\n",
      "Epoch 00015: loss improved from 1.79582 to 1.77399, saving model to model_weights.hdf5\n",
      "382/382 [==============================] - 988s 3s/step - loss: 1.7740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2489118a7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=15, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retraining the model\n",
    "model.load_weights(backup_file)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## opposite of char_to_num\n",
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" ong and how would feel with all their simple and find a pleasure in their simple remembering her own \"\n"
     ]
    }
   ],
   "source": [
    "## getting seed value\n",
    "start=np.random.randint(0,len(x_data)-1)\n",
    "pattern=x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\",''.join([num_to_char[x] for x in pattern]),\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and she said the she she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was the she was th\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#generating characters\n",
    "for i in range(500):\n",
    "    x=np.reshape(pattern,(1,len(pattern),1))\n",
    "    x=x/float(vocal_len)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = np.argmax(pred)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[x] for x in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]\n",
    "print(\"\\nDone\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
